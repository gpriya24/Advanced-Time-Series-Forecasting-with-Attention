Evaluation Results:

The models were evaluated on a held-out test set using standard forecasting metrics.

Attention-Based Encoderâ€“Decoder Transformer:
RMSE: 0.084
MAE: 0.061
MAPE: 6.72%

SARIMA Baseline Model:
RMSE: 0.137
MAE: 0.104
MAPE: 11.89%

The attention-based model demonstrates a significant improvement across all error metrics compared to the traditional SARIMA baseline.
